import streamlit as st
from supabase import create_client
import pandas as pd
from io import BytesIO
from openpyxl import load_workbook
import time
import requests
from urllib.parse import urlparse, urlunparse
from datetime import datetime

# -----------------------
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏ API –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
# -----------------------
st.set_page_config(page_title="SEO Index Manager", layout="wide")

# DataForSEO Constants
TASK_POST = "/v3/serp/google/organic/task_post"
TASKS_READY = "/v3/serp/google/organic/tasks_ready"
TASK_GET_ADV = "/v3/serp/google/organic/task_get/advanced/{task_id}"

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Supabase
@st.cache_resource
def init_supabase():
    url = st.secrets["supabase"]["url"]
    key = st.secrets["supabase"]["key"]
    return create_client(url, key)

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏ –¥–ª—è DataForSEO
def init_requests():
    s = requests.Session()
    s.auth = (st.secrets["dataforseo"]["login"], st.secrets["dataforseo"]["password"])
    s.headers.update({"Content-Type": "application/json"})
    return s

try:
    supabase = init_supabase()
except Exception as e:
    st.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
    st.stop()

# -----------------------
# –•–µ–ª–ø–µ—Ä—ã (–∏–∑ —Ç–≤–æ–µ–≥–æ —Å—Ç–∞—Ä–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞)
# -----------------------
def norm_url(u: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    p = urlparse(u.strip())
    netloc = (p.netloc or "").lower()
    if netloc.startswith("www."):
        netloc = netloc[4:]
    path = (p.path or "").rstrip("/")
    return urlunparse(("", netloc, path, "", "", "")).lower()

def build_site_query(url: str) -> str:
    """–°–æ–∑–¥–∞–µ—Ç –∑–∞–ø—Ä–æ—Å site:url"""
    p = urlparse(url.strip())
    host = (p.netloc or "").lower()
    if host.startswith("www."):
        host = host[4:]
    path = (p.path or "").strip().lstrip("/").rstrip("/")
    if path in ("", "/"):
        return f"site:{host}"
    return f"site:{host}/{path}"

def match_indexed(original_url: str, items):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ URL –≤ –≤—ã–¥–∞—á–µ"""
    orig = norm_url(original_url)
    for it in items:
        if it.get("type") == "organic":
            u = it.get("url")
            if u and norm_url(u) == orig:
                return True
    return False

def parse_excel_urls(uploaded_file):
    urls = []
    wb = load_workbook(BytesIO(uploaded_file.getvalue()), read_only=True)
    for ws in wb.worksheets:
        header_row = 1
        for r in range(1, 11):
            val = ws.cell(row=r, column=2).value
            if isinstance(val, str) and "referring page url" in val.lower():
                header_row = r
                break
        for r in range(header_row + 1, ws.max_row + 1):
            val = ws.cell(row=r, column=2).value
            if val and isinstance(val, str) and (val.startswith("http://") or val.startswith("https://")):
                urls.append(val.strip())
    return urls

# -----------------------
# –õ–æ–≥–∏–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ (Core Engine)
# -----------------------
def run_check(project_id, links_data):
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π links_data [{'id': 1, 'url': '...'}, ...]
    """
    session = init_requests()
    host = st.secrets["dataforseo"].get("host", "api.dataforseo.com").replace("https://", "")
    base_url = f"https://{host}"
    
    progress_bar = st.progress(0.0)
    status_text = st.empty()
    
    # 1. –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–¥–∞—á–∏ (POST)
    tasks_map = {} # task_id -> link_db_id
    payload = []
    
    # DataForSEO –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    post_body_base = {
        "location_code": 2840,
        "language_code": "en",
        "depth": 10
    }

    for item in links_data:
        p = post_body_base.copy()
        p["keyword"] = build_site_query(item['url'])
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º pingback_url –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ tag, —á—Ç–æ–±—ã —Å–≤—è–∑–∞—Ç—å –∑–∞–¥–∞—á—É. 
        # –ù–æ –ø—Ä–æ—â–µ —á–µ—Ä–µ–∑ –ø–æ—Ä—è–¥–æ–∫, —Ç–∞–∫ –∫–∞–∫ API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ.
        # –î–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –±—É–¥–µ–º –º–∞–ø–∏—Ç—å –ø–æ –ø–æ—Ä—è–¥–∫—É, –Ω–æ –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ.
        payload.append(p)

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –ø–æ 100, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ, –Ω–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –ø–æ–∫–∞ –æ–¥–Ω–∏–º –∫—É—Å–∫–æ–º (–¥–æ 100 —à—Ç)
    # –ï—Å–ª–∏ —Å—Å—ã–ª–æ–∫ > 100, –ª—É—á—à–µ –¥–æ–±–∞–≤–∏—Ç—å —Ü–∏–∫–ª –±–∞—Ç—á–∏–Ω–≥–∞. –î–æ–±–∞–≤–∏–º –ø—Ä–æ—Å—Ç–æ–π –±–∞—Ç—á–∏–Ω–≥.
    
    BATCH_SIZE = 50
    total = len(links_data)
    processed_count = 0
    
    for i in range(0, total, BATCH_SIZE):
        batch_links = links_data[i : i + BATCH_SIZE]
        batch_payload = payload[i : i + BATCH_SIZE]
        
        status_text.write(f"üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–¥–∞—á {i+1}-{min(i+BATCH_SIZE, total)} –∏–∑ {total}...")
        
        try:
            r = session.post(base_url + TASK_POST, json=batch_payload, timeout=60)
            res = r.json()
            if res.get('status_code') != 20000:
                st.error(f"API Error: {res.get('status_message')}")
                continue
                
            # –°–æ–±–∏—Ä–∞–µ–º ID –∑–∞–¥–∞—á
            batch_task_ids = []
            for idx, task in enumerate(res.get('tasks', [])):
                if task.get('id'):
                    tid = task['id']
                    # –°–≤—è–∑—ã–≤–∞–µ–º task_id —Å ID —Å—Å—ã–ª–∫–∏ –≤ –Ω–∞—à–µ–π –±–∞–∑–µ
                    link_db_id = batch_links[idx]['id']
                    tasks_map[tid] = link_db_id
                    batch_task_ids.append(tid)
                    
                    # (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ú–æ–∂–Ω–æ —Å—Ä–∞–∑—É –∑–∞–ø–∏—Å–∞—Ç—å task_id –≤ –±–∞–∑—É, —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å
            
            # 2. –ñ–¥–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            if not batch_task_ids:
                continue
                
            status_text.write("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç Google...")
            # –ü—Ä–æ—Å—Ç–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ (polling)
            completed_tasks = set()
            attempts = 0
            while len(completed_tasks) < len(batch_task_ids) and attempts < 20:
                time.sleep(3) 
                attempts += 1
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å (—É–ø—Ä–æ—â–µ–Ω–Ω–æ - —Å—Ä–∞–∑—É –ø—Ä–æ–±—É–µ–º GET, —Ç–∞–∫ –∫–∞–∫ task_post organic –æ–±—ã—á–Ω–æ –±—ã—Å—Ç—Ä, 
                # –Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–µ–µ —á–µ—Ä–µ–∑ tasks_ready. –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –∫–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º GET, –æ–Ω –≤–µ—Ä–Ω–µ—Ç 'status': 'working' –µ—Å–ª–∏ –Ω–µ –≥–æ—Ç–æ–≤)
                # –õ—É—á—à–µ –≤—Å–µ –∂–µ tasks_ready –¥–ª—è –±–∞—Ç—á–∞, –Ω–æ –¥–ª—è 50 —à—Ç—É–∫ –º–æ–∂–Ω–æ –∏ –≤ –ª–æ–±.
                pass 
            
            # 3. –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–µ
            for tid in batch_task_ids:
                # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                r_get = session.get(base_url + TASK_GET_ADV.format(task_id=tid), timeout=30)
                try:
                    d_get = r_get.json()
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≥–æ—Ç–æ–≤–∞ –ª–∏ –∑–∞–¥–∞—á–∞
                    task_res = (d_get.get('tasks') or [{}])[0]
                    
                    link_id = tasks_map[tid]
                    original_link_obj = next(l for l in batch_links if l['id'] == link_id)
                    
                    if task_res.get('status_code') == 20000:
                        result_items = (task_res.get('result') or [{}])[0].get('items', [])
                        is_ind = match_indexed(original_link_obj['url'], result_items)
                        
                        # –û–ë–ù–û–í–õ–Ø–ï–ú –ë–ê–ó–£
                        supabase.table("links").update({
                            "status": "done",
                            "is_indexed": is_ind,
                            "last_check": datetime.utcnow().isoformat(),
                            "task_id": tid
                        }).eq("id", link_id).execute()
                        
                    else:
                        # –û—à–∏–±–∫–∞ –∏–ª–∏ –µ—â–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
                        supabase.table("links").update({"status": "error"}).eq("id", link_id).execute()
                        
                except Exception as e:
                    print(f"Error parsing result: {e}")
        
            processed_count += len(batch_links)
            progress_bar.progress(processed_count / total)
            
        except Exception as e:
            st.error(f"–°–±–æ–π —Å–µ—Ç–∏ –∏–ª–∏ API: {e}")

    status_text.success("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    time.sleep(2)
    st.rerun()


# -----------------------
# –°–∞–π–¥–±–∞—Ä
# -----------------------
with st.sidebar:
    st.title("üóÇ –ú–æ–∏ –ü—Ä–æ–µ–∫—Ç—ã")
    
    with st.expander("‚ûï –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –ø–∞–ø–∫—É"):
        new_proj = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏")
        if st.button("–°–æ–∑–¥–∞—Ç—å"):
            if new_proj:
                supabase.table("projects").insert({"name": new_proj}).execute()
                st.rerun()

    st.divider()

    response = supabase.table("projects").select("*").order("created_at", desc=True).execute()
    projects = response.data
    
    selected_project_id = None
    if projects:
        opts = {p['name']: p['id'] for p in projects}
        p_name = st.selectbox("–ê–∫—Ç–∏–≤–Ω–∞—è –ø–∞–ø–∫–∞:", list(opts.keys()))
        selected_project_id = opts[p_name]

# -----------------------
# –û—Å–Ω–æ–≤–Ω–æ–π —ç–∫—Ä–∞–Ω
# -----------------------
if selected_project_id:
    st.title(f"üìÇ {p_name}")
    
    # –ì—Ä—É–∑–∏–º –¥–∞–Ω–Ω—ã–µ
    res = supabase.table("links").select("*").eq("project_id", selected_project_id).order("id", desc=False).execute()
    df = pd.DataFrame(res.data)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if not df.empty:
        total = len(df)
        indexed = len(df[df['is_indexed'] == True])
        pending = len(df[df['status'] == 'pending'])
        
        c1, c2, c3 = st.columns(3)
        c1.metric("–í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫", total)
        c2.metric("–í –∏–Ω–¥–µ–∫—Å–µ", indexed)
        c3.metric("–û—á–µ—Ä–µ–¥—å", pending)
        
        st.divider()
        
        # –ö–ù–û–ü–ö–ê –ó–ê–ü–£–°–ö–ê –ü–†–û–í–ï–†–ö–ò
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —á—Ç–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å (pending > 0)
        if pending > 0:
            if st.button(f"üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É ({pending} —à—Ç.)", type="primary"):
                # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ pending —Å—Å—ã–ª–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                links_to_check = df[df['status'] == 'pending'][['id', 'url']].to_dict('records')
                run_check(selected_project_id, links_to_check)
        else:
            if st.button("üîÑ –ü–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Å—ë (–°–±—Ä–æ—Å–∏—Ç—å —Å—Ç–∞—Ç—É—Å—ã)"):
                # –°–±—Ä–æ—Å —Å—Ç–∞—Ç—É—Å–æ–≤ –Ω–∞ pending
                supabase.table("links").update({
                    "status": "pending", 
                    "is_indexed": None
                }).eq("project_id", selected_project_id).execute()
                st.rerun()

    # –ó–∞–≥—Ä—É–∑–∫–∞
    with st.expander("üì• –î–æ–±–∞–≤–∏—Ç—å —Å—Å—ã–ª–∫–∏", expanded=(df.empty)):
        uploaded = st.file_uploader("Excel (–∫–æ–ª–æ–Ω–∫–∞ B)", type=["xlsx"])
        if uploaded and st.button("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å"):
            urls = parse_excel_urls(uploaded)
            if urls:
                data = [{"project_id": selected_project_id, "url": u, "status": "pending"} for u in urls]
                # Batch insert
                batch_size = 1000
                bar = st.progress(0)
                for i in range(0, len(data), batch_size):
                    supabase.table("links").insert(data[i:i+batch_size]).execute()
                    bar.progress(min((i+batch_size)/len(data), 1.0))
                st.success(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(urls)} —Å—Å—ã–ª–æ–∫")
                time.sleep(1)
                st.rerun()

    # –¢–∞–±–ª–∏—Ü–∞
    st.subheader("–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫")
    if not df.empty:
        st.dataframe(
            df[['url', 'status', 'is_indexed', 'last_check', 'created_at']], 
            use_container_width=True,
            column_config={
                "is_indexed": st.column_config.CheckboxColumn("Index?", disabled=True),
                "url": st.column_config.LinkColumn("URL")
            }
        )
    else:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö.")

else:
    st.write("–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–æ–µ–∫—Ç.")
