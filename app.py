import streamlit as st
from supabase import create_client
import pandas as pd
from io import BytesIO
from openpyxl import load_workbook
import time
import requests
from urllib.parse import urlparse, urlunparse
from datetime import datetime
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError

# -----------------------
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
# -----------------------
st.set_page_config(page_title="SEO Index Manager PRO", layout="wide")

# ==========================================
# –û–°–ù–û–í–ù–û–ï –ü–†–ò–õ–û–ñ–ï–ù–ò–ï
# ==========================================

TASK_POST = "/v3/serp/google/organic/task_post"
TASK_GET_ADV = "/v3/serp/google/organic/task_get/advanced/{task_id}"

@st.cache_resource
def init_supabase():
    url = st.secrets["supabase"]["url"]
    key = st.secrets["supabase"]["key"]
    return create_client(url, key)

def init_requests():
    s = requests.Session()
    s.auth = (st.secrets["dataforseo"]["login"], st.secrets["dataforseo"]["password"])
    s.headers.update({"Content-Type": "application/json"})
    return s

try:
    supabase = init_supabase()
except Exception as e:
    st.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
    st.stop()

# -----------------------
# –•–µ–ª–ø–µ—Ä—ã Slack
# -----------------------
def send_slack_file(file_bytes, filename, message):
    try:
        if "slack" in st.secrets:
            token = st.secrets["slack"].get("bot_token")
            channel = st.secrets["slack"].get("channel_id")
            
            if not token or not channel:
                st.error("‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Slack")
                return

            client = WebClient(token=token)
            client.files_upload_v2(
                channel=channel,
                file=file_bytes,
                filename=filename,
                title=filename,
                initial_comment=message
            )
            st.success("‚úÖ –û—Ç—á–µ—Ç —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ Slack!")
        else:
            st.warning("‚ö†Ô∏è –°–µ–∫—Ü–∏—è [slack] –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
    except SlackApiError as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ Slack API: {e.response['error']}")
    except Exception as e:
        st.error(f"‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")

# -----------------------
# –•–µ–ª–ø–µ—Ä—ã Excel (–ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê)
# -----------------------
def generate_project_report(project_id, project_name):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç Excel, –≥–¥–µ –∫–∞–∂–¥–∞—è –ü–ê–ü–ö–ê = –æ—Ç–¥–µ–ª—å–Ω—ã–π –õ–ò–°–¢.
    –°—Å—ã–ª–∫–∏ –±–µ–∑ –ø–∞–ø–∫–∏ –ø–æ–ø–∞–¥–∞—é—Ç –Ω–∞ –ª–∏—Å—Ç "General".
    """
    output = BytesIO()
    
    # 1. –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞
    links_res = supabase.table("links").select("*").eq("project_id", project_id).execute()
    df_links = pd.DataFrame(links_res.data)
    
    # 2. –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–∞–ø–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞
    folders_res = supabase.table("folders").select("*").eq("project_id", project_id).execute()
    df_folders = pd.DataFrame(folders_res.data)
    
    if df_links.empty:
        return None

    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        # --- –õ–ò–°–¢ 1: –°–≤–æ–¥–∫–∞ (Dashboard) ---
        summary_data = []
        if not df_folders.empty and not df_links.empty:
             for index, folder in df_folders.iterrows():
                 f_links = df_links[df_links['folder_id'] == folder['id']]
                 total = len(f_links)
                 indexed = len(f_links[f_links['is_indexed'] == True])
                 summary_data.append({
                     "–ü–∞–ø–∫–∞": folder['name'],
                     "–í—Å–µ–≥–æ": total,
                     "–í –∏–Ω–¥–µ–∫—Å–µ": indexed,
                     "%": f"{(indexed/total*100):.1f}%" if total > 0 else "0%"
                 })
             # –°—Å—ã–ª–∫–∏ –±–µ–∑ –ø–∞–ø–∫–∏
             gen_links = df_links[df_links['folder_id'].isnull()]
             if not gen_links.empty:
                 total = len(gen_links)
                 indexed = len(gen_links[gen_links['is_indexed'] == True])
                 summary_data.append({"–ü–∞–ø–∫–∞": "General (–ë–µ–∑ –ø–∞–ø–∫–∏)", "–í—Å–µ–≥–æ": total, "–í –∏–Ω–¥–µ–∫—Å–µ": indexed, "%": f"{(indexed/total*100):.1f}%" if total > 0 else "0%"})
             
             pd.DataFrame(summary_data).to_excel(writer, sheet_name="SUMMARY", index=False)

        # --- –õ–ò–°–¢–´ –ü–û –ü–ê–ü–ö–ê–ú ---
        # 1. –°—Å—ã–ª–∫–∏ —Å –ø–∞–ø–∫–∞–º–∏
        if not df_folders.empty:
            for index, folder in df_folders.iterrows():
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Å—ã–ª–∫–∏ —ç—Ç–æ–π –ø–∞–ø–∫–∏
                sub_df = df_links[df_links['folder_id'] == folder['id']]
                
                # –ò–º—è –ª–∏—Å—Ç–∞ (–æ—á–∏—Å—Ç–∫–∞ –æ—Ç —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤)
                sheet_name = "".join(c for c in folder['name'] if c.isalnum() or c in (' ', '_', '-'))[:30]
                if not sheet_name: sheet_name = f"Folder_{folder['id']}"
                
                if not sub_df.empty:
                    sub_df[['url', 'status', 'is_indexed', 'last_check']].to_excel(writer, index=False, sheet_name=sheet_name)
                else:
                    # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π –ª–∏—Å—Ç, —á—Ç–æ–±—ã —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∞—Å—å
                    pd.DataFrame({'Info': ['–ù–µ—Ç —Å—Å—ã–ª–æ–∫']}).to_excel(writer, index=False, sheet_name=sheet_name)
        
        # 2. –°—Å—ã–ª–∫–∏ –ë–ï–ó –ø–∞–ø–∫–∏ (General)
        general_df = df_links[df_links['folder_id'].isnull()]
        if not general_df.empty:
            general_df[['url', 'status', 'is_indexed', 'last_check']].to_excel(writer, index=False, sheet_name="General")

    return output.getvalue()

def to_simple_excel(df):
    """–ü—Ä–æ—Å—Ç–æ–π —ç–∫—Å–ø–æ—Ä—Ç —Ç–µ–∫—É—â–µ–π —Ç–∞–±–ª–∏—Ü—ã"""
    output = BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='List View')
    return output.getvalue()

# -----------------------
# –•–µ–ª–ø–µ—Ä—ã –ü–∞—Ä—Å–∏–Ω–≥–∞
# -----------------------
def norm_url(u: str) -> str:
    p = urlparse(u.strip())
    netloc = (p.netloc or "").lower()
    if netloc.startswith("www."): netloc = netloc[4:]
    path = (p.path or "").rstrip("/")
    return urlunparse(("", netloc, path, "", "", "")).lower()

def build_site_query(url: str) -> str:
    p = urlparse(url.strip())
    host = (p.netloc or "").lower()
    if host.startswith("www."): host = host[4:]
    path = (p.path or "").strip().lstrip("/").rstrip("/")
    return f"site:{host}" if path in ("", "/") else f"site:{host}/{path}"

def match_indexed(original_url: str, items):
    orig = norm_url(original_url)
    for it in items:
        if it.get("type") == "organic":
            u = it.get("url")
            if u and norm_url(u) == orig: return True
    return False

def parse_excel_with_folders(uploaded_file):
    """
    –ü–∞—Ä—Å–∏—Ç Excel. 
    –ö–æ–ª–æ–Ω–∫–∞ A = URL
    –ö–æ–ª–æ–Ω–∫–∞ B = –ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏ (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π: [{'url': '...', 'folder_name': '...'}]
    """
    data_list = []
    wb = load_workbook(BytesIO(uploaded_file.getvalue()), read_only=True)
    
    # –ò—â–µ–º –ø–µ—Ä–≤—ã–π –ª–∏—Å—Ç
    ws = wb.worksheets[0]
    
    # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ URL (–æ–±—ã—á–Ω–æ 1 —Å—Ç—Ä–æ–∫–∞, –Ω–æ –≤–¥—Ä—É–≥ —Å–º–µ—â–µ–Ω–∞)
    # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å–æ 2 —Å—Ç—Ä–æ–∫–∏, Col A=URL, Col B=Folder
    for r in range(1, ws.max_row + 1):
        url_val = ws.cell(row=r, column=1).value # A
        folder_val = ws.cell(row=r, column=2).value # B
        
        if url_val and isinstance(url_val, str) and (url_val.startswith("http://") or url_val.startswith("https://")):
            folder_name = str(folder_val).strip() if folder_val else None
            data_list.append({
                "url": url_val.strip(),
                "folder_name": folder_name
            })
            
    return data_list

def parse_text_urls(text_input):
    urls = []
    if not text_input: return urls
    lines = text_input.split('\n')
    for line in lines:
        line = line.strip()
        if line and (line.startswith("http://") or line.startswith("https://")):
            urls.append(line)
    return urls

# -----------------------
# –õ–æ–≥–∏–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ (Engine)
# -----------------------
def run_check(links_data, project_id=None, project_name="Unknown"):
    if not links_data: return
    session = init_requests()
    host = st.secrets["dataforseo"].get("host", "api.dataforseo.com").replace("https://", "")
    base_url = f"https://{host}"
    
    progress_bar = st.progress(0.0)
    status_text = st.empty()
    payload = []
    tasks_map = {} 
    
    for item in links_data:
        payload.append({
            "location_code": 2840,
            "language_code": "en",
            "depth": 10,
            "keyword": build_site_query(item['url'])
        })

    BATCH_SIZE = 50
    total = len(links_data)
    processed_count = 0
    count_indexed = 0
    count_not_indexed = 0
    
    for i in range(0, total, BATCH_SIZE):
        batch_links = links_data[i : i + BATCH_SIZE]
        batch_payload = payload[i : i + BATCH_SIZE]
        status_text.write(f"üì§ –û–±—Ä–∞–±–æ—Ç–∫–∞ {i+1}-{min(i+BATCH_SIZE, total)} –∏–∑ {total}...")
        
        try:
            r = session.post(base_url + TASK_POST, json=batch_payload, timeout=60)
            res = r.json()
            if res.get('status_code') == 20000:
                batch_task_ids = []
                for idx, task in enumerate(res.get('tasks', [])):
                    if task.get('id'):
                        tid = task['id']
                        link_db_id = batch_links[idx]['id']
                        tasks_map[tid] = link_db_id
                        batch_task_ids.append(tid)
                if not batch_task_ids: 
                    processed_count += len(batch_links)
                    continue
                
                time.sleep(2)
                status_text.write("‚è≥ –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
                
                for tid in batch_task_ids:
                    try:
                        r_get = session.get(base_url + TASK_GET_ADV.format(task_id=tid), timeout=30)
                        d_get = r_get.json()
                        link_id = tasks_map[tid]
                        original_link_obj = next(l for l in batch_links if l['id'] == link_id)
                        task_res = (d_get.get('tasks') or [{}])[0]
                        if task_res.get('status_code') == 20000:
                            result_items = (task_res.get('result') or [{}])[0].get('items', [])
                            is_ind = match_indexed(original_link_obj['url'], result_items)
                            if is_ind: count_indexed += 1
                            else: count_not_indexed += 1
                            supabase.table("links").update({
                                "status": "done",
                                "is_indexed": is_ind,
                                "last_check": datetime.utcnow().isoformat(),
                                "task_id": tid
                            }).eq("id", link_id).execute()
                        else:
                            supabase.table("links").update({"status": "error"}).eq("id", link_id).execute()
                    except Exception as e:
                        print(f"Err task {tid}: {e}")
            else:
                st.error(f"API Error: {res.get('status_message')}")
            
            processed_count += len(batch_links)
            progress_bar.progress(processed_count / total)
            
        except Exception as e:
            st.error(f"Network error: {e}")
        
        # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (Anti-Fraud)
        time.sleep(1.5)

    # === –û–¢–ü–†–ê–í–ö–ê –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–û–ì–û –û–¢–ß–ï–¢–ê ===
    status_text.write("üìä –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –ø–æ –ø–∞–ø–∫–∞–º...")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç —Å –≤–∫–ª–∞–¥–∫–∞–º–∏
    excel_bytes = generate_project_report(project_id, project_name)
    
    if excel_bytes:
        date_str = datetime.now().strftime('%Y-%m-%d')
        # –ò–º—è —Ñ–∞–π–ª–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ
        safe_proj_name = "".join(c for c in project_name if c.isalnum() or c in (' ', '_', '-'))[:20]
        fname = f"Report_{safe_proj_name}_{date_str}.xlsx"
        
        msg = f"‚úÖ *–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!*\nüìÇ –ü—Ä–æ–µ–∫—Ç: {project_name}\nüîó –í—Å–µ–≥–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ: {total}"
        send_slack_file(excel_bytes, fname, msg)
    else:
        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ.")
        
    time.sleep(2)
    st.rerun()

# -----------------------
# –°–ê–ô–î–ë–ê–†
# -----------------------
with st.sidebar:
    st.title("üóÇ –ú–µ–Ω—é –ø—Ä–æ–µ–∫—Ç–æ–≤")
    
    if st.button("üè† –î–æ–º–æ–π (–í—Å–µ –ø—Ä–æ–µ–∫—Ç—ã)", use_container_width=True):
        st.session_state.selected_project_id = None
        st.rerun()
    
    st.divider()
    
    st.subheader("–ú–æ–∏ –ü—Ä–æ–µ–∫—Ç—ã")
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞
    with st.expander("‚ûï –°–æ–∑–¥–∞—Ç—å –ü—Ä–æ–µ–∫—Ç"):
        new_proj = st.text_input("–ò–º—è –ø—Ä–æ–µ–∫—Ç–∞ (–Ω–∞–ø—Ä. Zoome AU)")
        if st.button("–°–æ–∑–¥–∞—Ç—å –ü—Ä–æ–µ–∫—Ç"):
            if new_proj:
                supabase.table("projects").insert({"name": new_proj}).execute()
                st.rerun()

    # –°–ø–∏—Å–æ–∫ –ø—Ä–æ–µ–∫—Ç–æ–≤
    response = supabase.table("projects").select("*").order("created_at", desc=True).execute()
    projects = response.data
    
    if "selected_project_id" not in st.session_state:
        st.session_state.selected_project_id = None

    if projects:
        for p in projects:
            is_active = (st.session_state.selected_project_id == p['id'])
            type_btn = "primary" if is_active else "secondary"
            # –ò–∫–æ–Ω–∫–∞
            if st.button(f"üìÇ {p['name']}", key=f"proj_{p['id']}", use_container_width=True, type=type_btn):
                st.session_state.selected_project_id = p['id']
                st.rerun()
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞
    if st.session_state.selected_project_id:
        st.divider()
        with st.expander("üóë –£–¥–∞–ª–∏—Ç—å –ø—Ä–æ–µ–∫—Ç"):
            st.warning("–£–¥–∞–ª—è—Ç—Å—è –≤—Å–µ –ø–∞–ø–∫–∏ –∏ —Å—Å—ã–ª–∫–∏ –≤–Ω—É—Ç—Ä–∏!")
            if st.button("–ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å —É–¥–∞–ª–µ–Ω–∏–µ", type="primary"):
                supabase.table("projects").delete().eq("id", st.session_state.selected_project_id).execute()
                st.session_state.selected_project_id = None
                st.rerun()

# -----------------------
# –õ–û–ì–ò–ö–ê –≠–ö–†–ê–ù–û–í
# -----------------------

# === –≠–ö–†–ê–ù –ü–†–û–ï–ö–¢–ê (–° –ü–ê–ü–ö–ê–ú–ò) ===
if st.session_state.selected_project_id:
    current_proj = next((p for p in projects if p['id'] == st.session_state.selected_project_id), None)
    if not current_proj:
        st.session_state.selected_project_id = None
        st.rerun()
        
    st.title(f"üìÇ –ü—Ä–æ–µ–∫—Ç: {current_proj['name']}")
    
    # 1. –ü–æ–ª—É—á–∞–µ–º –ü–ê–ü–ö–ò —ç—Ç–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞
    folders_res = supabase.table("folders").select("*").eq("project_id", current_proj['id']).order("created_at", desc=False).execute()
    folders = folders_res.data
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å {id: name} –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    folder_map = {f['id']: f['name'] for f in folders}
    folder_map[None] = "General (–ë–µ–∑ –ø–∞–ø–∫–∏)" # –î–ª—è —Å—Å—ã–ª–æ–∫ –±–µ–∑ –ø–∞–ø–∫–∏
    
    # 2. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–ø–∫–∞–º–∏
    col_new_folder, col_stats = st.columns([1, 2])
    with col_new_folder:
        with st.popover("‚ûï –î–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–ø–∞–ø–∫—É"):
            new_folder_name = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ –ø–æ–¥–ø–∞–ø–∫–∏ (–Ω–∞–ø—Ä. GP zoome17)")
            if st.button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–∞–ø–∫—É"):
                if new_folder_name:
                    supabase.table("folders").insert({"name": new_folder_name, "project_id": current_proj['id']}).execute()
                    st.rerun()
    
    # 3. –ü–æ–ª—É—á–∞–µ–º –í–°–ï —Å—Å—ã–ª–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞
    links_res = supabase.table("links").select("*").eq("project_id", current_proj['id']).execute()
    df = pd.DataFrame(links_res.data)

    # --- –¢–ê–ë–´: –°–í–û–î–ö–ê | –°–ü–ò–°–û–ö | –ó–ê–ì–†–£–ó–ö–ê ---
    tab_dashboard, tab_list, tab_upload = st.tabs(["üìä –°–≤–æ–¥–∫–∞", "üìù –°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫", "üì• –ó–∞–≥—Ä—É–∑–∏—Ç—å"])

    # --- TAB 1: DASHBOARD ---
    with tab_dashboard:
        if not df.empty:
            total_all = len(df)
            pending_all = len(df[df['status'] == 'pending'])
            
            # –ö–Ω–æ–ø–∫–∞ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
            if pending_all > 0:
                st.info(f"–í –æ—á–µ—Ä–µ–¥–∏ –Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫—É: {pending_all} —Å—Å—ã–ª–æ–∫ (–≤–æ –≤—Å–µ—Ö –ø–∞–ø–∫–∞—Ö).")
                if st.button(f"üöÄ –ó–ê–ü–£–°–¢–ò–¢–¨ –ü–†–û–í–ï–†–ö–£ ({pending_all} —à—Ç)", type="primary"):
                     to_check = df[df['status'] == 'pending'][['id', 'url']].to_dict('records')
                     run_check(to_check, project_id=current_proj['id'], project_name=current_proj['name'])
            
            # –ö–Ω–æ–ø–∫–∞ –ø–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä–∫–∏ –≤—Å–µ–≥–æ
            st.write("")
            if st.button(f"üîÑ –°–±—Ä–æ—Å–∏—Ç—å –∏ –ø–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä–∏—Ç—å –í–ï–°–¨ –ü–†–û–ï–ö–¢ ({total_all} —à—Ç)", type="secondary"):
                supabase.table("links").update({"status": "pending", "is_indexed": None}).eq("project_id", current_proj['id']).execute()
                to_check = df[['id', 'url']].to_dict('records')
                run_check(to_check, project_id=current_proj['id'], project_name=current_proj['name'])

            st.divider()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–∞–ø–∫–∞–º
            st.subheader("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–∞–ø–∫–∞–º")
            stats_data = []
            
            # –°—á–∏—Ç–∞–µ–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ø–∞–ø–æ–∫
            for f in folders:
                f_links = df[df['folder_id'] == f['id']]
                tot = len(f_links)
                ind = len(f_links[f_links['is_indexed'] == True])
                stats_data.append({
                    "–ü–∞–ø–∫–∞": f['name'],
                    "–°—Å—ã–ª–æ–∫": tot,
                    "–í –∏–Ω–¥–µ–∫—Å–µ": ind,
                    "%": f"{(ind/tot*100):.0f}%" if tot > 0 else "-"
                })
            
            # –°—á–∏—Ç–∞–µ–º –¥–ª—è General
            gen_links = df[df['folder_id'].isnull()]
            if not gen_links.empty:
                tot = len(gen_links)
                ind = len(gen_links[gen_links['is_indexed'] == True])
                stats_data.append({"–ü–∞–ø–∫–∞": "General (–ë–µ–∑ –ø–∞–ø–∫–∏)", "–°—Å—ã–ª–æ–∫": tot, "–í –∏–Ω–¥–µ–∫—Å–µ": ind, "%": f"{(ind/tot*100):.0f}%" if tot > 0 else "-"})
            
            st.dataframe(pd.DataFrame(stats_data), use_container_width=True, hide_index=True)

    # --- TAB 2: –°–ü–ò–°–û–ö ---
    with tab_list:
        if not df.empty:
            # –§–∏–ª—å—Ç—Ä –ø–æ –ø–∞–ø–∫–µ
            folder_options = ["–í—Å–µ"] + [f['name'] for f in folders] + ["General (–ë–µ–∑ –ø–∞–ø–∫–∏)"]
            selected_folder_filter = st.selectbox("–§–∏–ª—å—Ç—Ä –ø–æ –ø–∞–ø–∫–µ:", folder_options)
            
            df_view = df.copy()
            # –ú–∞–ø–ø–∏–Ω–≥ ID –ø–∞–ø–∫–∏ –≤ –ò–º—è –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã
            df_view['folder_name'] = df_view['folder_id'].map(folder_map)
            
            if selected_folder_filter != "–í—Å–µ":
                if selected_folder_filter == "General (–ë–µ–∑ –ø–∞–ø–∫–∏)":
                    df_view = df_view[df_view['folder_id'].isnull()]
                else:
                    # –ò—â–µ–º ID –ø–∞–ø–∫–∏ –ø–æ –∏–º–µ–Ω–∏
                    fid = next((f['id'] for f in folders if f['name'] == selected_folder_filter), None)
                    if fid:
                        df_view = df_view[df_view['folder_id'] == fid]

            # –¢–∞–±–ª–∏—Ü–∞
            selection = st.dataframe(
                df_view[['url', 'folder_name', 'status', 'is_indexed', 'last_check']],
                use_container_width=True,
                on_select="rerun",
                selection_mode="multi-row",
                column_config={
                    "is_indexed": st.column_config.CheckboxColumn("Index?", disabled=True),
                    "url": st.column_config.LinkColumn("URL"),
                    "folder_name": "–ü–∞–ø–∫–∞"
                }
            )
            
            # –î–µ–π—Å—Ç–≤–∏—è
            if len(selection.selection.rows) > 0:
                selected_ids = df_view.iloc[selection.selection.rows]['id'].tolist()
                st.info(f"–í—ã–±—Ä–∞–Ω–æ: {len(selected_ids)}")
                if st.button("üóë –£–¥–∞–ª–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ"):
                    supabase.table("links").delete().in_("id", selected_ids).execute()
                    st.rerun()

    # --- TAB 3: –ó–ê–ì–†–£–ó–ö–ê ---
    with tab_upload:
        st.info("üí° –ï—Å–ª–∏ –≤—ã –∑–∞–≥—Ä—É–∂–∞–µ—Ç–µ Excel, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ **–ö–æ–ª–æ–Ω–∫—É A** –¥–ª—è —Å—Å—ã–ª–æ–∫ –∏ **–ö–æ–ª–æ–Ω–∫—É B** –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏—è –ø–∞–ø–∫–∏. –ï—Å–ª–∏ –ø–∞–ø–∫–∏ –Ω–µ—Ç, –æ–Ω–∞ —Å–æ–∑–¥–∞—Å—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.")
        
        # 1. EXCEL
        uploaded = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç—å Excel (Col A: Url, Col B: Folder)", type=["xlsx"])
        if uploaded and st.button("üíæ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å Excel"):
            parsed_data = parse_excel_with_folders(uploaded) # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç [{'url':.., 'folder_name':..}]
            
            if parsed_data:
                # 1. –°–Ω–∞—á–∞–ª–∞ –Ω–∞–π–¥–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–¥–∏–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–∞–ø–∫–∏ –∏–∑ —Ñ–∞–π–ª–∞
                unique_folders = set(d['folder_name'] for d in parsed_data if d['folder_name'])
                folder_id_map = {f['name']: f['id'] for f in folders} # –¢–µ–∫—É—â–∏–µ –ø–∞–ø–∫–∏ {name: id}
                
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –ø–∞–ø–∫–∏
                for fname in unique_folders:
                    if fname not in folder_id_map:
                        res = supabase.table("folders").insert({"name": fname, "project_id": current_proj['id']}).execute()
                        if res.data:
                            folder_id_map[fname] = res.data[0]['id']
                
                # 2. –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ —Å—Å—ã–ª–æ–∫
                insert_rows = []
                for item in parsed_data:
                    fid = folder_id_map.get(item['folder_name']) if item['folder_name'] else None
                    insert_rows.append({
                        "project_id": current_proj['id'],
                        "url": item['url'],
                        "folder_id": fid,
                        "status": "pending"
                    })
                
                # 3. –í—Å—Ç–∞–≤–ª—è–µ–º –ø–∞—á–∫–∞–º–∏
                batch_size = 500
                bar = st.progress(0)
                for i in range(0, len(insert_rows), batch_size):
                    supabase.table("links").insert(insert_rows[i:i+batch_size]).execute()
                    bar.progress(min((i+batch_size)/len(insert_rows), 1.0))
                
                st.success(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(insert_rows)} —Å—Å—ã–ª–æ–∫!")
                time.sleep(1)
                st.rerun()

        st.divider()
        
        # 2. –¢–ï–ö–°–¢ (–° –≤—ã–±–æ—Ä–æ–º –ø–∞–ø–∫–∏)
        st.write("–†—É—á–Ω–æ–π –≤–≤–æ–¥:")
        target_folder = st.selectbox("–í –∫–∞–∫—É—é –ø–∞–ø–∫—É –¥–æ–±–∞–≤–∏—Ç—å?", ["General (–ë–µ–∑ –ø–∞–ø–∫–∏)"] + [f['name'] for f in folders])
        text_input = st.text_area("–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫:", height=100)
        
        if st.button("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–ø–∏—Å–æ–∫"):
            urls = parse_text_urls(text_input)
            if urls:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º ID –ø–∞–ø–∫–∏
                target_fid = None
                if target_folder != "General (–ë–µ–∑ –ø–∞–ø–∫–∏)":
                    target_fid = next((f['id'] for f in folders if f['name'] == target_folder), None)
                
                data = [{"project_id": current_proj['id'], "url": u, "folder_id": target_fid, "status": "pending"} for u in urls]
                supabase.table("links").insert(data).execute()
                st.success(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(urls)} —Å—Å—ã–ª–æ–∫ –≤ '{target_folder}'")
                time.sleep(1)
                st.rerun()

# === –ì–õ–ê–í–ù–´–ô –î–ê–®–ë–û–†–î (–í–°–ï –ü–†–û–ï–ö–¢–´) ===
else:
    st.title("üìä –û–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–æ–≤")
    
    # –ö—Ä–∞—Ç–∫–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Å–µ–º –ø—Ä–æ–µ–∫—Ç–∞–º
    if projects:
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å–∫–æ–ø–æ–º, —á—Ç–æ–±—ã –Ω–µ –¥–µ–ª–∞—Ç—å 100 –∑–∞–ø—Ä–æ—Å–æ–≤
        all_stats = []
        all_links_res = supabase.table("links").select("project_id, status").execute()
        df_all = pd.DataFrame(all_links_res.data)
        
        total_pending_global = 0
        
        for p in projects:
            if not df_all.empty:
                p_links = df_all[df_all['project_id'] == p['id']]
                cnt = len(p_links)
                pnd = len(p_links[p_links['status'] == 'pending'])
            else:
                cnt = 0
                pnd = 0
            
            total_pending_global += pnd
            all_stats.append({
                "–ü—Ä–æ–µ–∫—Ç": p['name'],
                "–°—Å—ã–ª–æ–∫": cnt,
                "–í –æ—á–µ—Ä–µ–¥–∏": pnd
            })
            
        m1, m2 = st.columns(2)
        m1.metric("–í—Å–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–æ–≤", len(projects))
        m2.metric("–û—á–µ—Ä–µ–¥—å (Global)", total_pending_global)
        
        st.dataframe(pd.DataFrame(all_stats), use_container_width=True, hide_index=True)
        
    else:
        st.info("–°–æ–∑–¥–∞–π—Ç–µ –ø–µ—Ä–≤—ã–π –ø—Ä–æ–µ–∫—Ç –≤ –º–µ–Ω—é —Å–ª–µ–≤–∞")
