import streamlit as st
from supabase import create_client
import pandas as pd
from io import BytesIO
from openpyxl import load_workbook
import time
import requests
from urllib.parse import urlparse, urlunparse
from datetime import datetime

# -----------------------
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
# -----------------------
st.set_page_config(page_title="SEO Index Manager", layout="wide")

# -----------------------
# üîê –°–ò–°–¢–ï–ú–ê –ê–í–¢–û–†–ò–ó–ê–¶–ò–ò
# -----------------------
def check_password():
    if "authenticated" not in st.session_state:
        st.session_state.authenticated = False
    
    # –ï—Å–ª–∏ —É–∂–µ –≤–æ—à–ª–∏ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
    if st.session_state.authenticated:
        return True

    # –≠–∫—Ä–∞–Ω –≤—Ö–æ–¥–∞
    st.title("üîí –í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É")
    password = st.text_input("–í–≤–µ–¥–∏—Ç–µ –ø–∞—Ä–æ–ª—å –¥–æ—Å—Ç—É–ø–∞", type="password")
    
    if st.button("–í–æ–π—Ç–∏"):
        # –ü–∞—Ä–æ–ª—å –±–µ—Ä–µ—Ç—Å—è –∏–∑ secrets.toml
        if password == st.secrets["auth"]["password"]:
            st.session_state.authenticated = True
            st.rerun()
        else:
            st.error("–ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å!")
    return False

# –ë–ª–æ–∫–∏—Ä—É–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, –ø–æ–∫–∞ –Ω–µ –≤–≤–µ–¥–µ–Ω –ø–∞—Ä–æ–ª—å
if not check_password():
    st.stop()

# ==========================================
# –û–°–ù–û–í–ù–û–ï –ü–†–ò–õ–û–ñ–ï–ù–ò–ï
# ==========================================

TASK_POST = "/v3/serp/google/organic/task_post"
TASK_GET_ADV = "/v3/serp/google/organic/task_get/advanced/{task_id}"

@st.cache_resource
def init_supabase():
    url = st.secrets["supabase"]["url"]
    key = st.secrets["supabase"]["key"]
    return create_client(url, key)

def init_requests():
    s = requests.Session()
    s.auth = (st.secrets["dataforseo"]["login"], st.secrets["dataforseo"]["password"])
    s.headers.update({"Content-Type": "application/json"})
    return s

try:
    supabase = init_supabase()
except Exception as e:
    st.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
    st.stop()

# -----------------------
# –•–µ–ª–ø–µ—Ä—ã
# -----------------------
def to_excel(df):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è DF –≤ Excel –±–∞–π—Ç—ã"""
    output = BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Report')
    return output.getvalue()

def norm_url(u: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    p = urlparse(u.strip())
    netloc = (p.netloc or "").lower()
    if netloc.startswith("www."): netloc = netloc[4:]
    path = (p.path or "").rstrip("/")
    return urlunparse(("", netloc, path, "", "", "")).lower()

def build_site_query(url: str) -> str:
    """–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ site: –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞"""
    p = urlparse(url.strip())
    host = (p.netloc or "").lower()
    if host.startswith("www."): host = host[4:]
    path = (p.path or "").strip().lstrip("/").rstrip("/")
    return f"site:{host}" if path in ("", "/") else f"site:{host}/{path}"

def match_indexed(original_url: str, items):
    """–ü–æ–∏—Å–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ –≤—ã–¥–∞—á–µ"""
    orig = norm_url(original_url)
    for it in items:
        if it.get("type") == "organic":
            u = it.get("url")
            if u and norm_url(u) == orig: return True
    return False

def parse_excel_urls(uploaded_file):
    """–ü–∞—Ä—Å–∏–Ω–≥ Excel (–∫–æ–ª–æ–Ω–∫–∞ B)"""
    urls = []
    wb = load_workbook(BytesIO(uploaded_file.getvalue()), read_only=True)
    for ws in wb.worksheets:
        header_row = 1
        for r in range(1, 11):
            val = ws.cell(row=r, column=2).value
            if isinstance(val, str) and "referring page url" in val.lower():
                header_row = r
                break
        for r in range(header_row + 1, ws.max_row + 1):
            val = ws.cell(row=r, column=2).value
            if val and isinstance(val, str) and (val.startswith("http://") or val.startswith("https://")):
                urls.append(val.strip())
    return urls

def parse_text_urls(text_input):
    """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Å—ã–ª–æ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—è"""
    urls = []
    if not text_input:
        return urls
    lines = text_input.split('\n')
    for line in lines:
        line = line.strip()
        if line and (line.startswith("http://") or line.startswith("https://")):
            urls.append(line)
    return urls

# -----------------------
# –õ–æ–≥–∏–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ (Engine)
# -----------------------
def run_check(links_data):
    """–ó–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ —Å–ª–æ–≤–∞—Ä–µ–π [{'id':..., 'url':...}]"""
    if not links_data: return
    
    session = init_requests()
    host = st.secrets["dataforseo"].get("host", "api.dataforseo.com").replace("https://", "")
    base_url = f"https://{host}"
    
    progress_bar = st.progress(0.0)
    status_text = st.empty()
    
    payload = []
    tasks_map = {} 
    
    # 1. –§–æ—Ä–º–∏—Ä—É–µ–º Payload
    for item in links_data:
        payload.append({
            "location_code": 2840,
            "language_code": "en",
            "depth": 10,
            "keyword": build_site_query(item['url'])
        })

    # 2. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞—á–∫–∞–º–∏ (Batching)
    BATCH_SIZE = 50
    total = len(links_data)
    processed_count = 0
    
    for i in range(0, total, BATCH_SIZE):
        batch_links = links_data[i : i + BATCH_SIZE]
        batch_payload = payload[i : i + BATCH_SIZE]
        
        status_text.write(f"üì§ –û–±—Ä–∞–±–æ—Ç–∫–∞ {i+1}-{min(i+BATCH_SIZE, total)} –∏–∑ {total}...")
        
        try:
            # POST
            r = session.post(base_url + TASK_POST, json=batch_payload, timeout=60)
            res = r.json()
            
            if res.get('status_code') == 20000:
                batch_task_ids = []
                for idx, task in enumerate(res.get('tasks', [])):
                    if task.get('id'):
                        tid = task['id']
                        link_db_id = batch_links[idx]['id']
                        tasks_map[tid] = link_db_id
                        batch_task_ids.append(tid)
                
                if not batch_task_ids: continue

                # Wait
                time.sleep(2)
                status_text.write("‚è≥ –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
                
                # GET Results
                for tid in batch_task_ids:
                    try:
                        r_get = session.get(base_url + TASK_GET_ADV.format(task_id=tid), timeout=30)
                        d_get = r_get.json()
                        
                        link_id = tasks_map[tid]
                        original_link_obj = next(l for l in batch_links if l['id'] == link_id)
                        
                        task_res = (d_get.get('tasks') or [{}])[0]
                        if task_res.get('status_code') == 20000:
                            result_items = (task_res.get('result') or [{}])[0].get('items', [])
                            is_ind = match_indexed(original_link_obj['url'], result_items)
                            
                            # UPDATE DB
                            supabase.table("links").update({
                                "status": "done",
                                "is_indexed": is_ind,
                                "last_check": datetime.utcnow().isoformat(),
                                "task_id": tid
                            }).eq("id", link_id).execute()
                        else:
                            supabase.table("links").update({"status": "error"}).eq("id", link_id).execute()
                            
                    except Exception as e:
                        print(f"Err task {tid}: {e}")
            else:
                st.error(f"API Error: {res.get('status_message')}")

            processed_count += len(batch_links)
            progress_bar.progress(processed_count / total)
            
        except Exception as e:
            st.error(f"Network error: {e}")

    status_text.success("‚úÖ –ì–æ—Ç–æ–≤–æ!")
    time.sleep(1)
    st.rerun()

# -----------------------
# –°–ê–ô–î–ë–ê–†
# -----------------------
with st.sidebar:
    st.title("üóÇ –ú–µ–Ω—é")
    
    if st.button("üè† –ù–∞ –≥–ª–∞–≤–Ω—É—é (–î–∞—à–±–æ—Ä–¥)", use_container_width=True):
        st.session_state.selected_project_id = None
        st.rerun()
    
    st.divider()
    
    st.subheader("–ú–æ–∏ –ü—Ä–æ–µ–∫—Ç—ã")
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞
    with st.expander("‚ûï –ù–æ–≤—ã–π –ø—Ä–æ–µ–∫—Ç"):
        new_proj = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ")
        if st.button("–°–æ–∑–¥–∞—Ç—å"):
            if new_proj:
                supabase.table("projects").insert({"name": new_proj}).execute()
                st.rerun()

    # –°–ø–∏—Å–æ–∫ –ø—Ä–æ–µ–∫—Ç–æ–≤
    response = supabase.table("projects").select("*").order("created_at", desc=True).execute()
    projects = response.data
    
    if "selected_project_id" not in st.session_state:
        st.session_state.selected_project_id = None

    if projects:
        for p in projects:
            is_active = (st.session_state.selected_project_id == p['id'])
            # –î–µ–ª–∞–µ–º –∞–∫—Ç–∏–≤–Ω—É—é –∫–Ω–æ–ø–∫—É –¥—Ä—É–≥–æ–≥–æ —Ü–≤–µ—Ç–∞
            type_btn = "primary" if is_active else "secondary"
            label = f"üìÇ {p['name']}"
            
            if st.button(label, key=f"proj_{p['id']}", use_container_width=True, type=type_btn):
                st.session_state.selected_project_id = p['id']
                st.rerun()
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞
    if st.session_state.selected_project_id:
        st.divider()
        with st.expander("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞"):
            st.caption("–û–ø–∞—Å–Ω–∞—è –∑–æ–Ω–∞")
            if st.button("üóë –£–¥–∞–ª–∏—Ç—å —ç—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç", type="primary"):
                try:
                    supabase.table("projects").delete().eq("id", st.session_state.selected_project_id).execute()
                    st.session_state.selected_project_id = None
                    st.success("–ü—Ä–æ–µ–∫—Ç —É–¥–∞–ª–µ–Ω!")
                    time.sleep(1)
                    st.rerun()
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è: {e}")

    # –í—ã—Ö–æ–¥
    st.write("")
    st.write("")
    st.divider()
    if st.button("üö™ –í—ã–π—Ç–∏", use_container_width=True):
        st.session_state.authenticated = False
        st.rerun()

# -----------------------
# –õ–û–ì–ò–ö–ê –≠–ö–†–ê–ù–û–í
# -----------------------

# 1. –≠–ö–†–ê–ù –ü–†–û–ï–ö–¢–ê (–î–µ—Ç–∞–ª—å–Ω—ã–π –≤–∏–¥)
if st.session_state.selected_project_id:
    current_proj = next((p for p in projects if p['id'] == st.session_state.selected_project_id), None)
    if not current_proj:
        st.session_state.selected_project_id = None
        st.rerun()
        
    st.title(f"üìÇ {current_proj['name']}")
    
    # –ì—Ä—É–∑–∏–º –¥–∞–Ω–Ω—ã–µ
    res = supabase.table("links").select("*").eq("project_id", st.session_state.selected_project_id).order("id", desc=False).execute()
    df = pd.DataFrame(res.data)

    if not df.empty:
        # –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        total = len(df)
        indexed = len(df[df['is_indexed'] == True])
        not_indexed = len(df[df['is_indexed'] == False])
        pending = len(df[df['status'] == 'pending'])
        
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("–í—Å–µ–≥–æ", total)
        c2.metric("–í –∏–Ω–¥–µ–∫—Å–µ", f"{indexed} ({(indexed/total*100):.1f}%)")
        c3.metric("–û—á–µ—Ä–µ–¥—å", pending)
        
        with c4:
            if pending > 0:
                if st.button("üöÄ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ—á–µ—Ä–µ–¥—å", type="primary"):
                    to_check = df[df['status'] == 'pending'][['id', 'url']].to_dict('records')
                    run_check(to_check)
            else:
                if st.button("üîÑ –ü–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Å—ë"):
                    supabase.table("links").update({"status": "pending", "is_indexed": None}).eq("project_id", st.session_state.selected_project_id).execute()
                    st.rerun()
        
        st.divider()

        # –§–∏–ª—å—Ç—Ä—ã –∏ –≠–∫—Å–ø–æ—Ä—Ç
        col_filter, col_export = st.columns([4, 1])
        with col_filter:
            filter_option = st.radio(
                "–§–∏–ª—å—Ç—Ä:",
                [f"–í—Å–µ ({total})", f"‚úÖ –í –∏–Ω–¥–µ–∫—Å–µ ({indexed})", f"‚ùå –ù–µ –≤ –∏–Ω–¥–µ–∫—Å–µ ({not_indexed})", f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ/–û—à–∏–±–∫–∏ ({pending})"],
                horizontal=True,
                label_visibility="collapsed"
            )
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä
            if "‚úÖ" in filter_option: df_view = df[df['is_indexed'] == True]
            elif "‚ùå" in filter_option: df_view = df[df['is_indexed'] == False]
            elif "‚è≥" in filter_option: df_view = df[df['status'].isin(['pending', 'error'])]
            else: df_view = df

        with col_export:
            excel_data = to_excel(df[['url', 'is_indexed', 'status', 'last_check']])
            st.download_button("üì• –°–∫–∞—á–∞—Ç—å –æ—Ç—á–µ—Ç", excel_data, f"report_{current_proj['name']}.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", use_container_width=True)
        
        # --- –¢–ê–ë–õ–ò–¶–ê –° –ú–£–õ–¨–¢–ò-–í–´–ë–û–†–û–ú ---
        st.write("") 
        selection = st.dataframe(
            df_view[['url', 'status', 'is_indexed', 'last_check', 'created_at']], 
            use_container_width=True,
            on_select="rerun", 
            selection_mode="multi-row", # –í–∫–ª—é—á–∞–µ–º –≥–∞–ª–æ—á–∫–∏
            column_config={
                "is_indexed": st.column_config.CheckboxColumn("Index?", disabled=True),
                "url": st.column_config.LinkColumn("URL"),
                "last_check": st.column_config.DatetimeColumn("–î–∞—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏", format="D MMM YYYY, HH:mm")
            }
        )
        
        # === –î–ï–ô–°–¢–í–ò–Ø –° –í–´–î–ï–õ–ï–ù–ù–´–ú ===
        if len(selection.selection.rows) > 0:
            selected_indices = selection.selection.rows
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ ID –≤—ã–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫
            selected_ids = df_view.iloc[selected_indices]['id'].tolist()
            count = len(selected_ids)
            
            st.info(f"–í—ã–±—Ä–∞–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {count}")
            
            b_col1, b_col2 = st.columns([1, 1])
            with b_col1:
                # üöÄ –ö–Ω–æ–ø–∫–∞ –≤—ã–±–æ—Ä–æ—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
                if st.button(f"üöÄ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ ({count})", type="primary", use_container_width=True):
                    # –°–±—Ä–æ—Å —Å—Ç–∞—Ç—É—Å–∞
                    supabase.table("links").update({"status": "pending", "is_indexed": None}).in_("id", selected_ids).execute()
                    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                    selected_records = df_view.iloc[selected_indices][['id', 'url']].to_dict('records')
                    run_check(selected_records)

            with b_col2:
                # üóë –ö–Ω–æ–ø–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è
                if st.button(f"üóë –£–¥–∞–ª–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ ({count})", type="secondary", use_container_width=True):
                    supabase.table("links").delete().in_("id", selected_ids).execute()
                    st.success("–£–¥–∞–ª–µ–Ω–æ!")
                    time.sleep(1)
                    st.rerun()

    else:
        st.info("–í –ø–∞–ø–∫–µ –ø—É—Å—Ç–æ.")
    
    # === –ë–õ–û–ö –ó–ê–ì–†–£–ó–ö–ò –°–°–´–õ–û–ö (Excel + –¢–µ–∫—Å—Ç) ===
    st.write("---")
    st.caption("–î–æ–±–∞–≤–∏—Ç—å —Å—Å—ã–ª–∫–∏")
    
    # –°–æ–∑–¥–∞–µ–º –≤–∫–ª–∞–¥–∫–∏
    tab1, tab2 = st.tabs(["üìÅ –ó–∞–≥—Ä—É–∑–∏—Ç—å Excel", "üìù –í—Å—Ç–∞–≤–∏—Ç—å —Å–ø–∏—Å–∫–æ–º (–¢–µ–∫—Å—Ç)"])
    
    with tab1:
        uploaded = st.file_uploader("–§–∞–π–ª .xlsx (—Å—Å—ã–ª–∫–∏ –≤ –∫–æ–ª–æ–Ω–∫–µ B)", type=["xlsx"])
        if uploaded and st.button("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å Excel"):
            urls = parse_excel_urls(uploaded)
            if urls:
                data = [{"project_id": st.session_state.selected_project_id, "url": u, "status": "pending"} for u in urls]
                batch_size = 1000
                bar = st.progress(0)
                for i in range(0, len(data), batch_size):
                    supabase.table("links").insert(data[i:i+batch_size]).execute()
                    bar.progress(min((i+batch_size)/len(data), 1.0))
                st.success(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(urls)} —Å—Å—ã–ª–æ–∫")
                time.sleep(1)
                st.rerun()
    
    with tab2:
        # –¢–µ–∫—Å—Ç–æ–≤–æ–µ –ø–æ–ª–µ –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –≤–≤–æ–¥–∞
        text_input = st.text_area("–í—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫–∏ (–∫–∞–∂–¥–∞—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏):", height=150, placeholder="https://site.com/page1\nhttps://site.com/page2")
        if st.button("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–ø–∏—Å–æ–∫"):
            urls = parse_text_urls(text_input)
            if urls:
                data = [{"project_id": st.session_state.selected_project_id, "url": u, "status": "pending"} for u in urls]
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—á–∫–∞–º–∏
                batch_size = 1000
                for i in range(0, len(data), batch_size):
                    supabase.table("links").insert(data[i:i+batch_size]).execute()
                
                st.success(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(urls)} —Å—Å—ã–ª–æ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞")
                time.sleep(1)
                st.rerun()
            else:
                if text_input: st.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ (–¥–æ–ª–∂–Ω—ã –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å http/https)")

# 2. –ì–õ–ê–í–ù–´–ô –î–ê–®–ë–û–†–î (–û–±–∑–æ—Ä –≤—Å–µ—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤)
else:
    st.title("üìä –î–∞—à–±–æ—Ä–¥ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞")
    
    # –ì—Ä—É–∑–∏–º —Å–≤–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    all_links_res = supabase.table("links").select("id, project_id, status, is_indexed, last_check, url").execute()
    all_links_df = pd.DataFrame(all_links_res.data)
    
    if projects:
        stats_data = []
        global_pending_count = 0
        
        for p in projects:
            pid = p['id']
            if not all_links_df.empty:
                p_links = all_links_df[all_links_df['project_id'] == pid]
                total = len(p_links)
                idx = len(p_links[p_links['is_indexed'] == True])
                pend = len(p_links[p_links['status'] == 'pending'])
                last_date = pd.to_datetime(p_links['last_check']).max() if not p_links['last_check'].isna().all() else None
            else:
                total, idx, pend, last_date = 0, 0, 0, None
            
            global_pending_count += pend
            
            stats_data.append({
                "ID": pid,
                "–ü—Ä–æ–µ–∫—Ç": p['name'],
                "–í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫": total,
                "–í –∏–Ω–¥–µ–∫—Å–µ": idx,
                "% Index": f"{(idx/total*100):.1f}%" if total > 0 else "0%",
                "–û—á–µ—Ä–µ–¥—å": pend,
                "–ü–æ—Å–ª–µ–¥–Ω—è—è –ø—Ä–æ–≤–µ—Ä–∫–∞": last_date
            })
            
        stats_df = pd.DataFrame(stats_data)
        
        m1, m2 = st.columns([3, 1])
        m1.metric("–í—Å–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–æ–≤", len(projects))
        m2.metric("–í—Å–µ–≥–æ –∑–∞–¥–∞—á –≤ –æ—á–µ—Ä–µ–¥–∏", global_pending_count)
        
        # –ö–Ω–æ–ø–∫–∞ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
        if global_pending_count > 0:
            st.warning(f"–ù–∞–π–¥–µ–Ω–æ {global_pending_count} —Å—Å—ã–ª–æ–∫ –æ–∂–∏–¥–∞—é—â–∏—Ö –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–æ –≤—Å–µ—Ö –ø–∞–ø–∫–∞—Ö.")
            if st.button(f"üöÄ –ó–ê–ü–£–°–¢–ò–¢–¨ –í–°–ï ({global_pending_count} —à—Ç.)", type="primary", use_container_width=True):
                pending_all = all_links_df[all_links_df['status'] == 'pending'][['id', 'url']].to_dict('records')
                run_check(pending_all)
        else:
            st.success("–í—Å–µ —Å—Å—ã–ª–∫–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã! –û—á–µ—Ä–µ–¥—å –ø—É—Å—Ç–∞.")
            
        st.subheader("–°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞")
        st.dataframe(stats_df, use_container_width=True, hide_index=True)
    else:
        st.info("–°–æ–∑–¥–∞–π—Ç–µ –ø–µ—Ä–≤—ã–π –ø—Ä–æ–µ–∫—Ç –≤ –º–µ–Ω—é —Å–ª–µ–≤–∞!")
